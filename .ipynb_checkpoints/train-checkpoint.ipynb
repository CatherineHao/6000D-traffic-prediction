{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d594f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e689f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes in this cell is proposed in Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting, Sijie Yan, Yuanjun Xiong and Dahua Lin, AAAI 2018.\n",
    "# The key structure of spatio-temporal graph convolutional networks.\n",
    "# sources: https://github.com/hazdzz/STGCN/models/layers.py\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "class Align(nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super(Align, self).__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.align_conv = nn.Conv2d(in_channels=c_in, out_channels=c_out, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.c_in > self.c_out:\n",
    "            x = self.align_conv(x)\n",
    "        elif self.c_in < self.c_out:\n",
    "            batch_size, _, timestep, n_vertex = x.shape\n",
    "            x = torch.cat([x, torch.zeros([batch_size, self.c_out - self.c_in, timestep, n_vertex]).to(x)], dim=1)\n",
    "        else:\n",
    "            x = x\n",
    "        \n",
    "        return x\n",
    "\n",
    "class CausalConv1d(nn.Conv1d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, enable_padding=False, dilation=1, groups=1, bias=True):\n",
    "        if enable_padding == True:\n",
    "            self.__padding = (kernel_size - 1) * dilation\n",
    "        else:\n",
    "            self.__padding = 0\n",
    "        super(CausalConv1d, self).__init__(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=self.__padding, dilation=dilation, groups=groups, bias=bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        result = super(CausalConv1d, self).forward(input)\n",
    "        if self.__padding != 0:\n",
    "            return result[: , : , : -self.__padding]\n",
    "        \n",
    "        return result\n",
    "\n",
    "class CausalConv2d(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, enable_padding=False, dilation=1, groups=1, bias=True):\n",
    "        kernel_size = nn.modules.utils._pair(kernel_size)\n",
    "        stride = nn.modules.utils._pair(stride)\n",
    "        dilation = nn.modules.utils._pair(dilation)\n",
    "        if enable_padding == True:\n",
    "            self.__padding = [int((kernel_size[i] - 1) * dilation[i]) for i in range(len(kernel_size))]\n",
    "        else:\n",
    "            self.__padding = 0\n",
    "        self.left_padding = nn.modules.utils._pair(self.__padding)\n",
    "        super(CausalConv2d, self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=0, dilation=dilation, groups=groups, bias=bias)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if self.__padding != 0:\n",
    "            input = F.pad(input, (self.left_padding[1], 0, self.left_padding[0], 0))\n",
    "        result = super(CausalConv2d, self).forward(input)\n",
    "\n",
    "        return result\n",
    "\n",
    "class TemporalConvLayer(nn.Module):\n",
    "\n",
    "    # Temporal Convolution Layer (GLU)\n",
    "    #\n",
    "    #        |--------------------------------| * Residual Connection *\n",
    "    #        |                                |\n",
    "    #        |    |--->--- CasualConv2d ----- + -------|       \n",
    "    # -------|----|                                   ⊙ ------>\n",
    "    #             |--->--- CasualConv2d --- Sigmoid ---|                               \n",
    "    #\n",
    "    \n",
    "    #param x: tensor, [bs, c_in, ts, n_vertex]\n",
    "\n",
    "    def __init__(self, Kt, c_in, c_out, n_vertex, act_func):\n",
    "        super(TemporalConvLayer, self).__init__()\n",
    "        self.Kt = Kt\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.n_vertex = n_vertex\n",
    "        self.align = Align(c_in, c_out)\n",
    "        if act_func == 'glu' or act_func == 'gtu':\n",
    "            self.causal_conv = CausalConv2d(in_channels=c_in, out_channels=2 * c_out, kernel_size=(Kt, 1), enable_padding=False, dilation=1)\n",
    "        else:\n",
    "            self.causal_conv = CausalConv2d(in_channels=c_in, out_channels=c_out, kernel_size=(Kt, 1), enable_padding=False, dilation=1)\n",
    "        self.act_func = act_func\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.silu = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):   \n",
    "        x_in = self.align(x)[:, :, self.Kt - 1:, :]\n",
    "        x_causal_conv = self.causal_conv(x)\n",
    "\n",
    "        if self.act_func == 'glu' or self.act_func == 'gtu':\n",
    "            x_p = x_causal_conv[:, : self.c_out, :, :]\n",
    "            x_q = x_causal_conv[:, -self.c_out:, :, :]\n",
    "\n",
    "            if self.act_func == 'glu':\n",
    "                # GLU was first purposed in\n",
    "                # *Language Modeling with Gated Convolutional Networks*.\n",
    "                # URL: https://arxiv.org/abs/1612.08083\n",
    "                # Input tensor X is split by a certain dimension into tensor X_a and X_b.\n",
    "                # In PyTorch, GLU is defined as X_a ⊙ Sigmoid(X_b).\n",
    "                # URL: https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.glu\n",
    "                # (x_p + x_in) ⊙ Sigmoid(x_q)\n",
    "                x = torch.mul((x_p + x_in), self.sigmoid(x_q))\n",
    "\n",
    "            else:\n",
    "                # Tanh(x_p + x_in) ⊙ Sigmoid(x_q)\n",
    "                x = torch.mul(self.tanh(x_p + x_in), self.sigmoid(x_q))\n",
    "\n",
    "        elif self.act_func == 'relu':\n",
    "            x = self.relu(x_causal_conv + x_in)\n",
    "        \n",
    "        elif self.act_func == 'leaky_relu':\n",
    "            x = self.leaky_relu(x_causal_conv + x_in)\n",
    "\n",
    "        elif self.act_func == 'silu':\n",
    "            x = self.silu(x_causal_conv + x_in)\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(f'ERROR: The activation function {self.act_func} is not implemented.')\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ChebGraphConv(nn.Module):\n",
    "    def __init__(self, c_in, c_out, Ks, gso, bias):\n",
    "        super(ChebGraphConv, self).__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.Ks = Ks\n",
    "        self.gso = gso\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(Ks, c_in, c_out))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(c_out))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #bs, c_in, ts, n_vertex = x.shape\n",
    "        x = torch.permute(x, (0, 2, 3, 1))\n",
    "\n",
    "        if self.Ks - 1 < 0:\n",
    "            raise ValueError(f'ERROR: the graph convolution kernel size Ks has to be a positive integer, but received {self.Ks}.')  \n",
    "        elif self.Ks - 1 == 0:\n",
    "            x_0 = x\n",
    "            x_list = [x_0]\n",
    "        elif self.Ks - 1 == 1:\n",
    "            x_0 = x\n",
    "            x_1 = torch.einsum('hi,btij->bthj', self.gso, x)\n",
    "            x_list = [x_0, x_1]\n",
    "        elif self.Ks - 1 >= 2:\n",
    "            x_0 = x\n",
    "            x_1 = torch.einsum('hi,btij->bthj', self.gso, x)\n",
    "            x_list = [x_0, x_1]\n",
    "            for k in range(2, self.Ks):\n",
    "                x_list.append(torch.einsum('hi,btij->bthj', 2 * self.gso, x_list[k - 1]) - x_list[k - 2])\n",
    "        \n",
    "        x = torch.stack(x_list, dim=2)\n",
    "\n",
    "        cheb_graph_conv = torch.einsum('btkhi,kij->bthj', x, self.weight)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            cheb_graph_conv = torch.add(cheb_graph_conv, self.bias)\n",
    "        else:\n",
    "            cheb_graph_conv = cheb_graph_conv\n",
    "        \n",
    "        return cheb_graph_conv\n",
    "\n",
    "class GraphConv(nn.Module):\n",
    "    def __init__(self, c_in, c_out, gso, bias):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.gso = gso\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(c_in, c_out))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(c_out))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #bs, c_in, ts, n_vertex = x.shape\n",
    "        x = torch.permute(x, (0, 2, 3, 1))\n",
    "\n",
    "        first_mul = torch.einsum('hi,btij->bthj', self.gso, x)\n",
    "        second_mul = torch.einsum('bthi,ij->bthj', first_mul, self.weight)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            graph_conv = torch.add(second_mul, self.bias)\n",
    "        else:\n",
    "            graph_conv = second_mul\n",
    "        \n",
    "        return graph_conv\n",
    "\n",
    "class GraphConvLayer(nn.Module):\n",
    "    def __init__(self, graph_conv_type, c_in, c_out, Ks, gso, bias):\n",
    "        super(GraphConvLayer, self).__init__()\n",
    "        self.graph_conv_type = graph_conv_type\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.align = Align(c_in, c_out)\n",
    "        self.Ks = Ks\n",
    "        self.gso = gso\n",
    "        if self.graph_conv_type == 'cheb_graph_conv':\n",
    "            self.cheb_graph_conv = ChebGraphConv(c_out, c_out, Ks, gso, bias)\n",
    "        elif self.graph_conv_type == 'graph_conv':\n",
    "            self.graph_conv = GraphConv(c_out, c_out, gso, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_gc_in = self.align(x)\n",
    "        if self.graph_conv_type == 'cheb_graph_conv':\n",
    "            x_gc = self.cheb_graph_conv(x_gc_in)\n",
    "        elif self.graph_conv_type == 'graph_conv':\n",
    "            x_gc = self.graph_conv(x_gc_in)\n",
    "        x_gc = x_gc.permute(0, 3, 1, 2)\n",
    "        x_gc_out = torch.add(x_gc, x_gc_in)\n",
    "\n",
    "        return x_gc_out\n",
    "\n",
    "class STConvBlock(nn.Module):\n",
    "    # STConv Block contains 'TGTND' structure\n",
    "    # T: Gated Temporal Convolution Layer (GLU or GTU)\n",
    "    # G: Graph Convolution Layer (ChebGraphConv or GraphConv)\n",
    "    # T: Gated Temporal Convolution Layer (GLU or GTU)\n",
    "    # N: Layer Normolization\n",
    "    # D: Dropout\n",
    "\n",
    "    def __init__(self, Kt, Ks, n_vertex, last_block_channel, channels, act_func, graph_conv_type, gso, bias, droprate):\n",
    "        super(STConvBlock, self).__init__()\n",
    "        self.tmp_conv1 = TemporalConvLayer(Kt, last_block_channel, channels[0], n_vertex, act_func)\n",
    "        self.graph_conv = GraphConvLayer(graph_conv_type, channels[0], channels[1], Ks, gso, bias)\n",
    "        self.tmp_conv2 = TemporalConvLayer(Kt, channels[1], channels[2], n_vertex, act_func)\n",
    "        self.tc2_ln = nn.LayerNorm([n_vertex, channels[2]])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=droprate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tmp_conv1(x)\n",
    "        x = self.graph_conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.tmp_conv2(x)\n",
    "        x = self.tc2_ln(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class OutputBlock(nn.Module):\n",
    "    # Output block contains 'TNFF' structure\n",
    "    # T: Gated Temporal Convolution Layer (GLU or GTU)\n",
    "    # N: Layer Normolization\n",
    "    # F: Fully-Connected Layer\n",
    "    # F: Fully-Connected Layer\n",
    "\n",
    "    def __init__(self, Ko, last_block_channel, channels, end_channel, n_vertex, act_func, bias, droprate):\n",
    "        super(OutputBlock, self).__init__()\n",
    "        self.tmp_conv1 = TemporalConvLayer(Ko, last_block_channel, channels[0], n_vertex, act_func)\n",
    "        self.fc1 = nn.Linear(in_features=channels[0], out_features=channels[1], bias=bias)\n",
    "        self.fc2 = nn.Linear(in_features=channels[1], out_features=end_channel, bias=bias)\n",
    "        self.tc1_ln = nn.LayerNorm([n_vertex, channels[0]])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.silu = nn.SiLU()\n",
    "        self.dropout = nn.Dropout(p=droprate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tmp_conv1(x)\n",
    "        x = self.tc1_ln(x.permute(0, 2, 3, 1))\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x).permute(0, 3, 1, 2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "610fefd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time2vec(x, f, w0, b0, w, b):\n",
    "    # One Non-periodic feature\n",
    "    v1 = torch.matmul(x, w0) + b0\n",
    "    # k-1 periodic features\n",
    "    v2 = f(torch.matmul(x, w) + b)\n",
    "    return torch.cat([v1, v2], dim=2)\n",
    "\n",
    "class Time2Vec(nn.Module):\n",
    "    '''\n",
    "    Using sine from time point to vector, which consists of 2 parts:\n",
    "    1. periodic 2.non-periodic\n",
    "    x: bz, days, stations\n",
    "    :return bz, stations, days, embeding_dim(out_features)\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features, bz, stations,days):\n",
    "        super(Time2Vec, self).__init__()\n",
    "        self.w0 = nn.parameter.Parameter(torch.randn(bz, days, in_features))\n",
    "        self.b0 = nn.parameter.Parameter(torch.randn(bz, stations, in_features))\n",
    "        self.w = nn.parameter.Parameter(torch.randn(bz, days, out_features - 1))\n",
    "        self.b = nn.parameter.Parameter(torch.randn(bz, stations, out_features - 1))\n",
    "        self.f = torch.sin\n",
    "\n",
    "    def forward(self, x):\n",
    "        return time2vec(x, self.f, self.w0, self.b0, self.w, self.b)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    '''\n",
    "    use Transformer to model the same stop in different days\n",
    "    :x bz, days, stations\n",
    "    '''\n",
    "    def __init__(self, time2vector, in_features, out_features, bz, stations, days):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.bz = bz\n",
    "        self.stations = stations\n",
    "        self.embedding = nn.Linear(days, out_features)\n",
    "        self.T2V = time2vector(in_features, out_features, bz, stations, days)\n",
    "        self.positional_embedding = self.T2V\n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=out_features, nhead=8, dropout=0.1)\n",
    "        self.decoder = nn.TransformerEncoder(self.encoder, num_layers=3)\n",
    "        self.linear = nn.Linear(out_features, out_features*2)\n",
    "        nn.init.kaiming_normal_(self.linear.weight, mode='fan_in')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # X [bz, days, stations]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x_input = self.embedding(x)                             # bz, stations, 29->d_models(out_features)\n",
    "        x_pos = self.positional_embedding(x)                    # bz, stations, out_features\n",
    "        x_time = x_input + x_pos                                # bz, stations, out_features\n",
    "        x_out = torch.zeros_like(x_time)\n",
    "        for i in range(self.stations):                          # self.stations = x_time.shape[1]\n",
    "            x_feature = self.decoder(x_time[:, i, :])           # bz, stations, out_features\n",
    "            x_out[:, i, :] = x_feature\n",
    "        return self.linear(x_out)                               # bz, stations, out_features*2\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, out_dim, days):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        assert self.hidden_dim == self.out_dim\n",
    "        self.lstm = nn.LSTM(self.hidden_dim, self.out_dim, num_layers=3, batch_first=True)\n",
    "        self.linear = nn.Linear(self.out_dim, 1)\n",
    "        nn.init.kaiming_normal_(self.linear.weight, mode='fan_in')\n",
    "\n",
    "    def forward(self, x):\n",
    "        teacher_forcing_ratio = 0.5\n",
    "        input = x[:, 0, :]\n",
    "        out = torch.zeros_like(x)\n",
    "        for i in range(x.shape[1]):\n",
    "            out[:, i, :], _ = self.lstm(input)\n",
    "            input = out[:, i, :] + (x[:, i, :] - out[:, i, :]) * teacher_forcing_ratio\n",
    "        output = self.linear(out)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CausalConv1d(torch.nn.Conv1d):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride=1,\n",
    "                 dilation=1,\n",
    "                 groups=1,\n",
    "                 bias=True):\n",
    "\n",
    "        super(CausalConv1d, self).__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=0,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=bias)\n",
    "\n",
    "        self.__padding = (kernel_size - 1) * dilation\n",
    "\n",
    "    def forward(self, input):\n",
    "        return super(CausalConv1d, self).forward(F.pad(input, (self.__padding, 0)))\n",
    "\n",
    "\n",
    "class TemperalModel(nn.Module):\n",
    "    def __init__(self, Transformer, time2vector, LSTM, bz, stations, days, in_features, out_features, hidden_dim, out_dim):\n",
    "        super(TemperalModel, self).__init__()\n",
    "        # self.T = Transformer(time2vector, in_features, out_features, bz, stations, days)\n",
    "        self.embedding = nn.Linear(days, out_features)\n",
    "        self.T = time2vector(in_features, out_features, bz, stations, days)\n",
    "        self.L = LSTM(hidden_dim, out_dim, days)\n",
    "        self.layer_norm = nn.LayerNorm(out_features)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        T_params = sum(p.numel() for p in self.T.parameters() if p.requires_grad)\n",
    "        L_params = sum(p.numel() for p in self.L.parameters() if p.requires_grad)\n",
    "        return T_params+L_params\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.T(x)              # bz, stations, out_features\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.L(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "264cd67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils as utils\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tqdm\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import json\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "layout = {\n",
    "    'PLOT': {\n",
    "        \"loss\": [\"Multiline\", ['Loss/train', 'Loss/val']],\n",
    "        \"Metrics\": [\"Multiline\", ['Metrics/mae', 'Metrics/lr']],\n",
    "    },\n",
    "}\n",
    "writer = SummaryWriter()\n",
    "writer.add_custom_scalars(layout)\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=30, verbose=False, delta=0, path=\"./checkpoint/A-best_model.pth\", trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 30\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'Early stopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "    \n",
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Read data from a file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    return data\n",
    "\n",
    "def z_score(x):\n",
    "    '''\n",
    "    Z-score normalization function: $z = (X - \\mu) / \\sigma $,\n",
    "    where $\\mu$ and $\\sigma$ are the mean and standard deviation of the data.\n",
    "    '''\n",
    "    mean, std = torch.std_mean(x)\n",
    "    x_out = (x - mean) / std\n",
    "    zero_value = torch.min(x_out)\n",
    "    return x_out, mean, std, zero_value\n",
    "\n",
    "def convert_str_to_list(line):\n",
    "    \"\"\"\n",
    "    Convert string to list.\n",
    "    \"\"\"\n",
    "    return np.array(json.loads(line))\n",
    "\n",
    "def generate_data(datafile, max_len=50):\n",
    "    \"\"\"\n",
    "    Generate data. txt --> list --> tensor(padding to max_len)\n",
    "    return: [len(data), 30, max_len]\n",
    "    \"\"\"\n",
    "    data = read_data(datafile)\n",
    "    data_input = torch.zeros(len(data), 30, max_len)\n",
    "    num = 0\n",
    "    for line in data:\n",
    "        line = convert_str_to_list(line)\n",
    "        route_len = int(len(line) / 30)\n",
    "        data_line = line.reshape(30, route_len)\n",
    "        data_line = torch.from_numpy(data_line)\n",
    "        data_line[25, :] = (data_line[24, :] + data_line[26, :])/2\n",
    "        data_line = F.pad(input=data_line, pad=(0, max_len - route_len, 0, 0), mode='constant', value=0)\n",
    "        data_input[num] = data_line\n",
    "        num += 1\n",
    "    return data_input\n",
    "\n",
    "def data_transform(datafile, max_len, batch_size):\n",
    "    \"\"\"\n",
    "    produce data slices for x_data and y_data\n",
    "    :param data: [len(data), 30, max_len]\n",
    "    :return: x:[len(data), :29, max_len] , y:[len(data), 30, max_len]\n",
    "    \"\"\"\n",
    "    data = generate_data(datafile, max_len)\n",
    "    data, mean, std, zero_value = z_score(data)\n",
    "    x = torch.zeros([len(data), 29, max_len])\n",
    "    y = torch.zeros([len(data),  1, max_len])\n",
    "    # mask_x = torch.zeros([len(data), 29, max_len])\n",
    "    # mask_y = torch.zeros([len(data),  1, max_len])\n",
    "    mask_x = torch.empty([len(data), 29, max_len])\n",
    "    mask_y = torch.zeros([len(data), 1, max_len])\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        x[i] = data[i][:29]   # [1,29,50]\n",
    "        y[i] = data[i][29]    # [1,1,50]\n",
    "        # a = torch.logical_not(x[i] == zero_value)\n",
    "        mask_x[i] = torch.logical_not(x[i] == zero_value)\n",
    "        mask_y[i] = torch.logical_not(y[i] == zero_value)\n",
    "\n",
    "    # data = utils.data.TensorDataset(x.cuda(), y.cuda(), mask_x.cuda(), mask_y.cuda())\n",
    "    data = utils.data.TensorDataset(x.cuda(), y.cuda())\n",
    "    iter_data = utils.data.DataLoader(dataset=data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    return iter_data, zero_value, mean, std\n",
    "\n",
    "parser = argparse.ArgumentParser(description='BUS')\n",
    "parser.add_argument('--enable_cuda', type=bool, default='True', help='enable CUDA, default as True')\n",
    "parser.add_argument('--seed', type=int, default=42, help='set the random seed for stabilize experiment results')\n",
    "parser.add_argument('--kt', type=int, default=3, help='kenel size of temporal convolution')\n",
    "parser.add_argument('--ks', type=int, default=3, help='kenel size of spatial convolution')\n",
    "parser.add_argument('--epoches', type=int, default=2000)\n",
    "parser.add_argument('--step_size', type=int, default=10)\n",
    "parser.add_argument('--patience', type=int, default=50, help='early stop,How long to wait after last time validation loss improved.')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--loss', type=str, default='mse', choices=['mse'])\n",
    "parser.add_argument('--bz', type=int, default=10, help='batchsize')\n",
    "parser.add_argument('--max_len', type=int, default=50, help='the max length of route')\n",
    "parser.add_argument('--days', type=int, default=29)\n",
    "parser.add_argument('--in_f', type=int, default=1)\n",
    "parser.add_argument('--out_f', type=int, default=256)\n",
    "parser.add_argument('--hidden_dim', type=int, default=256)\n",
    "parser.add_argument('--out_dim', type=int, default=256)\n",
    "# 从pycharm处移植代码到jupyter会出现参数错误：An exception has occurred, use %tb to see the full traceback.\n",
    "parser.add_argument('-f',type=str, default=\"读取额外的参数\")\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18706bc9",
   "metadata": {},
   "source": [
    "import the used dataset and environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "def set_env(seed):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False ## find fittest convolution\n",
    "    torch.backends.cudnn.deterministic = True ## keep experiment result stable\n",
    "set_env(seed=args.seed)\n",
    "\n",
    "model = TemperalModel(Transformer, Time2Vec, LSTM, bz=args.bz, stations=args.max_len, days=args.days,\n",
    "                      in_features=args.in_f, out_features=args.out_f, hidden_dim=args.hidden_dim, out_dim=args.out_dim).to(device)\n",
    "print(model.count_parameters()) # 1781505\n",
    "\n",
    "trainFile = './data_used/train.txt'\n",
    "valFile = './data_used/val.txt'\n",
    "testFile = './data_used/test.txt'\n",
    "train_iter, zero_value1, mean1, std1 = data_transform(trainFile, max_len=args.max_len, batch_size=args.bz)\n",
    "val_iter, zero_value2, mean2, std2 = data_transform(valFile, max_len=args.max_len, batch_size=args.bz)\n",
    "test_iter, zero_value3, mean3, std3 = data_transform(testFile, max_len=args.max_len, batch_size=args.bz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c32ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler1 = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "scheduler2 = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1.2)\n",
    "scheduler3 = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "scheduler4 = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1.2)\n",
    "scheduler5 = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "scheduler6 = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1.2)\n",
    "scheduler7 = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "scheduler8 = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1.2)\n",
    "scheduler9 = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "scheduler10 = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1.2)\n",
    "scheduler11 = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a8dfeb",
   "metadata": {},
   "source": [
    "train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e387cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, optimizer, scheduler, loss, early_stopping, model, train_iter, val_iter):\n",
    "    min_val_loss = np.inf\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        l_sum = []\n",
    "        for x, y in tqdm.tqdm(train_iter):\n",
    "            # print(x.shape, y.shape,)\n",
    "            y_pred = model(x).permute(0, 2, 1)\n",
    "            mask = torch.logical_not(y == zero_value1)\n",
    "            y_pred_without_padding = torch.masked_select(y_pred, mask)\n",
    "            y_without_padding = torch.masked_select(y, mask)\n",
    "            # print(y.shape, y_pred.shape)\n",
    "            # print(y_pred_without_padding.shape, y_without_padding.shape)\n",
    "            l = loss(y_pred_without_padding, y_without_padding)\n",
    "            writer.add_scalar(\"Loss/train\", np.mean(l.item()), epoch)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            # print(l)\n",
    "            l_sum.append(np.mean(l.item()))\n",
    "        if epoch <= 100:\n",
    "            scheduler1.step()\n",
    "        elif epoch > 100 and epoch <= 200:\n",
    "            scheduler2.step()\n",
    "        else:\n",
    "            scheduler3.step()\n",
    "        val_loss, mae = val(model, val_iter, epoch=epoch)\n",
    "        writer.add_scalar(\"Metrics/lr\", optimizer.param_groups[0]['lr'], epoch)\n",
    "        writer.add_scalar(\"Metrics/mae\", mae, epoch)\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "        # early_stopping(mae, model)\n",
    "        gpu_mem_alloc = torch.cuda.max_memory_allocated() / 1000000 if torch.cuda.is_available() else 0\n",
    "        print('Epoch: {:03d} | Lr: {:.6f} |Train loss: {:.6f} | Val loss: {:.6f} | MAE: {:.4f} |GPU occupy: {:.6f} MiB'. \\\n",
    "              format(epoch + 1, optimizer.param_groups[0]['lr'], np.mean(l_sum), val_loss, mae, gpu_mem_alloc))\n",
    "\n",
    "        torch.save(model.state_dict(), './model_mid/'+str(epoch+1)+'-'+str(mae)+'_model.pth')\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "    print('\\nTraining finished.\\n')\n",
    "    \n",
    "def val(model, val_loader, mean=mean2, std=std2, epoch=0):\n",
    "    model.eval()\n",
    "    l_sum = []\n",
    "    total_y_pred, total_y = [], []\n",
    "    with torch.no_grad():\n",
    "        mae, sum_y, mape, mse = [], [], [], []\n",
    "        for x, y in val_loader:\n",
    "            y_pred = model(x).permute(0, 2, 1)\n",
    "            mask = torch.logical_not(y == zero_value2)\n",
    "            y_pred_without_padding = torch.masked_select(y_pred, mask).cpu()\n",
    "            y_without_padding = torch.masked_select(y, mask).cpu()\n",
    "            l = loss(y_pred_without_padding, y_without_padding)\n",
    "            l_sum.append(np.mean(l.item()))\n",
    "            writer.add_scalar(\"Loss/val\", np.mean(l.item()), epoch)\n",
    "            ### score\n",
    "            y_without_padding = mean + std * y_without_padding\n",
    "            y_pred_without_padding = mean + std * y_pred_without_padding\n",
    "            d = np.abs(y_without_padding - y_pred_without_padding)/60\n",
    "            mae += d.tolist()\n",
    "            sum_y += y_without_padding.tolist()\n",
    "            mape += (d / y_without_padding).tolist()\n",
    "            mse += (d ** 2).tolist()\n",
    "        MAE = np.array(mae).mean()\n",
    "        RMSE = np.sqrt(np.array(mse).mean())\n",
    "        WMAPE = np.sum(np.array(mae)) / np.sum(np.array(sum_y))\n",
    "        return np.mean(l_sum), MAE\n",
    "\n",
    "def test(model_save_path, model, test_loader, score):\n",
    "    model.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1509d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env(args.seed)\n",
    "train(args.epoches, optimizer, scheduler1, loss, EarlyStopping(patience=args.patience), model, train_iter, val_iter)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d56709",
   "metadata": {},
   "source": [
    "KNN train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "def euclidean_dist(o1, o2):\n",
    "    '''\n",
    "    Calculate the euclidean distance between two objects\n",
    "    :param o1: [[x1,y1],[x2,y2],[x3,y3]...]\n",
    "    :param o2:\n",
    "    :return:\n",
    "    '''\n",
    "    dist = []\n",
    "    for i in range(len(o1)):\n",
    "        x1, y1 = o1[i][0], o1[i][1]\n",
    "        x2, y2 = o2[i][0], o2[i][1]\n",
    "        dist.append(np.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2))\n",
    "    return np.mean(dist)\n",
    "\n",
    "\n",
    "def knn_model(data, k, order, trip):\n",
    "    data_c = data.copy()\n",
    "    data_c.coord = data_c.coord.apply(lambda x: x[:order])\n",
    "    # print(len(data.coord[0]))\n",
    "    dist_all = []\n",
    "    for i in range(len(data_c)):\n",
    "        dist = euclidean_dist(data_c.coord[i], trip)\n",
    "        dist_all.append(dist)\n",
    "    dist_top_k = np.argsort(dist_all)[:k]\n",
    "    # print(dist_top_k)\n",
    "    return dist_top_k\n",
    "\n",
    "\n",
    "def knn_predict(data, dist_top_k, order):\n",
    "    predict_candidates = []\n",
    "    for i in dist_top_k:\n",
    "        predict_candidates.append(data.coord[i][order][1])\n",
    "    return np.mean(predict_candidates)\n",
    "\n",
    "\n",
    "l = [380,\n",
    " 200,\n",
    " 280,\n",
    " 470,\n",
    " 2890,\n",
    " 780,\n",
    " 750,\n",
    " 560,\n",
    " 560,\n",
    " 320,\n",
    " 280,\n",
    " 240,\n",
    " 1460,\n",
    " 460,\n",
    " 690,\n",
    " 380,\n",
    " 570,\n",
    " 320,\n",
    " 360,\n",
    " 620,\n",
    " 440,\n",
    " 510,\n",
    " 1330,\n",
    " 1270,\n",
    " 1320,\n",
    " 550,\n",
    " 350,\n",
    " 380,\n",
    " 410,\n",
    " 340,\n",
    " 640,\n",
    " 220,\n",
    " 360,\n",
    " 410,\n",
    " 780,\n",
    " 140,\n",
    " 70]\n",
    "l = [0]+[int(i/10) for i in l]\n",
    "l = np.cumsum(l)\n",
    "\n",
    "data = pd.read_csv('./data_used/knn_dataset/train.csv')\n",
    "data['coord'] = data['coord'].apply(eval)\n",
    "train, test = train_test_split(data, test_size=0.1, random_state=42)\n",
    "kf5 = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "for train_index, test_index in kf5.split(data):\n",
    "    train, test = data.iloc[train_index], data.iloc[test_index]\n",
    "train, test = train.reset_index(drop=True), test.reset_index(drop=True)\n",
    "predict_list = []\n",
    "error_list = []\n",
    "truth_list = []\n",
    "for sample in tqdm.tqdm(range(len(test))):\n",
    "    predict_sample = []\n",
    "    error_sample = []\n",
    "    truth_sample = []\n",
    "    for i in range(2, len(l)):\n",
    "        trip = test['coord'][sample]\n",
    "        dist_top_k = knn_model(train, k=25, order=4*i, trip=trip[:4*i])\n",
    "        predict = knn_predict(train, dist_top_k, 4*i)\n",
    "        predict_sample.append(predict)\n",
    "        error = np.abs(trip[4*i][1] - predict)/60\n",
    "        error_sample.append(error)\n",
    "        truth_sample.append(trip[4*i][1])\n",
    "    predict_list.append(predict_sample)\n",
    "    error_list.append(error_sample)\n",
    "    truth_list.append(truth_sample)\n",
    "\n",
    "with open('./result/knn-predict.txt', 'wb') as handle:\n",
    "    pickle.dump(predict_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('./result/knn-mae.txt', 'wb') as handle:\n",
    "    pickle.dump(error_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('./result/knn-truth.txt', 'wb') as handle:\n",
    "    pickle.dump(truth_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "error_list = np.array(error_list)\n",
    "error_list = np.mean(error_list, axis=0)\n",
    "# plot\n",
    "plt.stem(error_list, linefmt='b-', markerfmt='bo', basefmt='r-')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
